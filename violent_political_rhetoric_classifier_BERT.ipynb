{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"violent_political_rhetoric_classifier_BERT.ipynb","provenance":[{"file_id":"138tULktfzWm7G5GmsJbPodq1aHJvXqUc","timestamp":1609339940203},{"file_id":"1JKQj-DWHLv_vBdF3VypAIEC6npULOFGy","timestamp":1598840250112},{"file_id":"1O7PrfpCZk5th4f9y43vczpMiyI15OgYh","timestamp":1575217299548}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LXiC0F-y_2Bc"},"source":["Author notes (Taegyoon Kim, taegyoon@psu.edu)\n","\n","\n","---\n","\n","\n","- This is a notebook for a BERT classifier for violent political rhetoric (https://osf.io/5ckw4/). \n","- The input data is tweet text that contains one or more of the violent keywords extracted using the violent keyword extractor (https://github.com/taegyoon-kim/violent_political_rheotric_on_twitter/blob/master/violent_political_rhetoric_violent_keyword_extract.py). \n","- The training data is available upon request via email. \n","- The notebook will be fully available upon publication of the paper\n","- The classifier is trained on GPU provided Google Colaboratory\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I1UmhiynTVYF"},"source":["Drive mount\n","\n","---\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"IchOpHbOWgwo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624848867028,"user_tz":-540,"elapsed":23962,"user":{"displayName":"Taegyoon Kim","photoUrl":"","userId":"13025568659711700824"}},"outputId":"853e088d-20b9-4f42-abf5-3e0f60418fd9"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bNfWpwgX5Yan"},"source":["Packages\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"ZEQWPt7bg2Ek"},"source":["# the classifier is bsed on simpletransformers package https://github.com/ThilinaRajapakse/simpletransformers\n","\n","!pip install torch torchvision\n","!pip install transformers==2.10.0\n","!pip install seqeval\n","!pip install tensorboardx\n","!pip install simpletransformers==0.9.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_UAgUZkgqJm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624848896607,"user_tz":-540,"elapsed":5126,"user":{"displayName":"Taegyoon Kim","photoUrl":"","userId":"13025568659711700824"}},"outputId":"2bf61083-b8c7-460a-978c-8653a7e948c8"},"source":["%matplotlib inline\n","\n","import pandas as pd\n","import numpy as np\n","\n","import gc\n","import requests\n","import os\n","\n","from simpletransformers.classification import ClassificationModel\n","from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, recall_score, precision_score\n","from scipy.special import softmax\n","\n","import torch\n","print(\"Cuda available\" if torch.cuda.is_available() is True else \"CPU\")\n","print(\"PyTorch version: \", torch.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cuda available\n","PyTorch version:  1.9.0+cu102\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d4k5z4eI51Q7"},"source":["Load data\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"Mj5WtN6LnTzI"},"source":["url= '' # training set\n","\n","df = pd.read_csv(url)\n","df['text'] = df['status_final_text']\n","df['threat'] = df['final_binary'].astype(float)\n","df = df[['text','threat']]\n","\n","df = df.sample(frac=1).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2iGMeXa7TmB0"},"source":["Performance metrics\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"1xzlfdywQDGm"},"source":["def report_results(A, B):\n","    A_name = A.name\n","    B_name = B.name\n","    \n","    df = pd.DataFrame({'A':A,\n","                       'B':B})\n","    df = df.dropna()\n","    A = df['A']\n","    B = df['B']\n","    \n","    acc = accuracy_score(B, A)\n","    f1 = f1_score(B, A)\n","    prec = precision_score(B, A)\n","    rec = recall_score(B, A)\n","    ROC = roc_auc_score(B, A)\n","    \n","    print('Candidate: '+A_name+' | Ground Truth: '+B_name+'\\n')\n","    print('accuracy: %0.4f \\nprecision: %0.4f \\nrecall: %0.4f \\nF1 score: %0.4f \\nROC AUC: %0.4f \\n' % (acc, prec, rec, f1, ROC))\n","\n","    performance = [prec, rec, f1, ROC, acc]\n","\n","    return performance"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kdmb_JZ6TtPg"},"source":["5-fold Cross-validation\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"oHbJu47MOsGl"},"source":["args = {\n","   'output_dir': 'outputs/',\n","   'cache_dir': 'cache/',\n","\n","   'fp16': False,\n","   'fp16_opt_level': 'O1',\n","   'max_seq_length': 250,\n","   'train_batch_size': 8,\n","   'eval_batch_size': 8,\n","   'gradient_accumulation_steps': 1,\n","   'num_train_epochs': 2,\n","   'weight_decay': 0,\n","   'learning_rate': 4e-5,\n","   'adam_epsilon': 1e-8,\n","   'warmup_ratio': 0.06,\n","   'warmup_steps': 0,\n","   'max_grad_norm': 1.0,\n","\n","   'logging_steps': 50,\n","   'evaluate_during_training': False,\n","   'save_steps': 2000,\n","   'eval_all_checkpoints': True,\n","   'use_tensorboard': True,\n","\n","   'overwrite_output_dir': True,\n","   'reprocess_input_data': True   \n","}\n","\n","from sklearn.model_selection import KFold\n","n = 5\n","kf = KFold(n_splits = n, random_state = 7, shuffle = True)\n","\n","for train_index, val_index in kf.split(df):\n","  # splitting Dataframe (dataset not included)\n","    train_df = df.iloc[train_index]\n","    val_df = df.iloc[val_index]\n","  # Defining Model\n","    model = ClassificationModel('bert', 'bert-base-uncased', args = args)\n","  # train the model\n","    model.train_model(train_df)\n","  # validate the model \n","    #result, model_outputs, wrong_predictions = model.eval_model(val_df)\n","    #val_df['BERT_threat'] = np.argmax(model_outputs, axis = 1)\n","    predictions, raw_outputs = model.predict(val_df['text'])\n","    probabilities = softmax(raw_outputs, axis=1)\n","    val_df['BERT_threat_900'] = np.where(probabilities[:,1] >= 0.9, 1, 0)\n","    val_df['BERT_threat_925'] = np.where(probabilities[:,1] >= 0.925, 1, 0)\n","    val_df['BERT_threat_950'] = np.where(probabilities[:,1] >= 0.95, 1, 0)\n","  # performance\n","    performance_900 = report_results(val_df['BERT_threat_900'], val_df['threat'])\n","    performance_925 = report_results(val_df['BERT_threat_925'], val_df['threat'])\n","    performance_950 = report_results(val_df['BERT_threat_950'], val_df['threat'])\n","    print(performance_900)\n","    print(performance_925)\n","    print(performance_950)"],"execution_count":null,"outputs":[]}]}