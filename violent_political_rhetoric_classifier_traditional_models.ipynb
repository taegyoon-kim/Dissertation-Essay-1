{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"violent_political_rhetoric_classifier_traditional_models.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1FJJtvPusNumN7NhE2kFAv1gleu03GBSy","authorship_tag":"ABX9TyNAkuJJV2SgSGKhQgbi6Aol"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PZMh6SGOWs1B"},"source":["**1. Packages and drive mount**"]},{"cell_type":"code","metadata":{"id":"roT24CLcQgsh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624848414562,"user_tz":-540,"elapsed":365,"user":{"displayName":"Taegyoon Kim","photoUrl":"","userId":"13025568659711700824"}},"outputId":"f787caa6-995d-436c-9817-aa19d44e3eda"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IOfXYk2GN19z","executionInfo":{"status":"ok","timestamp":1624848417537,"user_tz":-540,"elapsed":484,"user":{"displayName":"Taegyoon Kim","photoUrl":"","userId":"13025568659711700824"}}},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn import model_selection\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split, cross_val_score \n","from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score\n","from sklearn.model_selection import cross_val_score"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8YypKtlW9cy"},"source":["**2. Load data**"]},{"cell_type":"code","metadata":{"id":"WtBCLaSpOCM-","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1624848453829,"user_tz":-540,"elapsed":618,"user":{"displayName":"Taegyoon Kim","photoUrl":"","userId":"13025568659711700824"}},"outputId":"dfc1e950-ad92-4b8b-8ca3-4c045e5bd48e"},"source":["url = \"https://raw.githubusercontent.com/taegyoon-kim/Dissertation-Essay-1/master/training_sep21.csv\" # the URL for labeled data\n","\n","df = pd.read_csv(url)\n","df['text'] = df['status_final_text']\n","df['threat'] = df['final_binary'].astype(float)\n","df = df[['text','threat']]\n","\n","df = df.sample(frac=1).reset_index(drop=True)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>threat</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>More money to law enforcement to improve it. I...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>@realDonaldTrump @SenatorTimScott How are you ...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@donwinslow @BGHeaven Fuck trump and may he di...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>.@SenKamalaHarris is gonna eat him alive, chew...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@tonyposnanski @realDonaldTrump His death is f...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>10092</th>\n","      <td>@SenatorRounds Lets bomb them into Religious F...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>10093</th>\n","      <td>So what. Every president in history has had an...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>10094</th>\n","      <td>@DubP23 @bennyjohnson @Jim_Jordan Are you sayi...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>10095</th>\n","      <td>@mcuban @tedcruz He doesn't have balls. He gav...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>10096</th>\n","      <td>@authenticmts @RandPaul You burned him good on...</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10097 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                                                    text  threat\n","0      More money to law enforcement to improve it. I...     0.0\n","1      @realDonaldTrump @SenatorTimScott How are you ...     0.0\n","2      @donwinslow @BGHeaven Fuck trump and may he di...     1.0\n","3      .@SenKamalaHarris is gonna eat him alive, chew...     0.0\n","4      @tonyposnanski @realDonaldTrump His death is f...     0.0\n","...                                                  ...     ...\n","10092  @SenatorRounds Lets bomb them into Religious F...     0.0\n","10093  So what. Every president in history has had an...     0.0\n","10094  @DubP23 @bennyjohnson @Jim_Jordan Are you sayi...     0.0\n","10095  @mcuban @tedcruz He doesn't have balls. He gav...     0.0\n","10096  @authenticmts @RandPaul You burned him good on...     0.0\n","\n","[10097 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"Y59x5GNpcKLe"},"source":["**3. Evaluation metrics**"]},{"cell_type":"code","metadata":{"id":"rNjBNKgwcQoV","executionInfo":{"status":"ok","timestamp":1624848465809,"user_tz":-540,"elapsed":726,"user":{"displayName":"Taegyoon Kim","photoUrl":"","userId":"13025568659711700824"}}},"source":["from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n","\n","def report_results(A, B):\n","    \n","    df = pd.DataFrame({'A':A,\n","                       'B':B})\n","    df = df.dropna()\n","    A = df['A']\n","    B = df['B']\n","    \n","    acc = accuracy_score(B, A)\n","    f1 = f1_score(B, A)\n","    prec = precision_score(B, A)\n","    rec = recall_score(B, A)\n","\n","    performance = [acc, prec, rec, f1]\n","\n","    return performance"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qa6UUfrFXGHb"},"source":["**4. Logistic regression + Count Vector & TF-IDF Vector & Word Embeddings**"]},{"cell_type":"code","metadata":{"id":"XF4JYfchSYc7"},"source":["##### count vector\n","\n","\n","count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True) # you can cahnge arguments here depending on how you want the text to be pre-processed/represented as a matrix\n","train_val_X = count.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1) # you set the number of folds for cross-validation\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  lr = LogisticRegression(C=1, random_state=7, solver='sag', max_iter=2000, n_jobs=-1) # this is where your can change hyper-parameters for logistic regression\n","  lr.fit(train_X, train_y)\n","  pred_y = lr.predict(val_X)\n","  \n","  lr_performance = report_results(pred_y, val_y)\n","  cv_acc.append(lr_performance[0])\n","  cv_pre.append(lr_performance[1])\n","  cv_rec.append(lr_performance[2])\n","  cv_f1.append(lr_performance[3])\n","\n","print('\\nLR + count')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### TFIDF vector\n","\n","\n","tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","train_val_X = tfidf.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  lr = LogisticRegression(C=1, random_state=7, solver='sag', max_iter=2000, n_jobs=-1)\n","  lr.fit(train_X, train_y)\n","  pred_y = lr.predict(val_X)\n","  \n","  lr_performance = report_results(pred_y, val_y)\n","  cv_acc.append(lr_performance[0])\n","  cv_pre.append(lr_performance[1])\n","  cv_rec.append(lr_performance[2])\n","  cv_f1.append(lr_performance[3])\n","\n","print('\\nLR + tfidf')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### glove\n","\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","embeddings_index = {}\n","f = open('glove.6B.200d.txt', encoding=\"utf8\")  # this is where glove embedding file is stored\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","\n","glove_X = [sent2vec(x) for x in tqdm(df[\"text\"])]\n","glove_X = np.array(glove_X)\n","y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  train_X = glove_X[train_index,:]\n","  val_X = glove_X[val_index,:]\n","\n","  train_y = y[train_index]\n","  val_y = y[val_index]\n","\n","  lr = LogisticRegression(C=1, random_state=7, solver='sag', max_iter=2000, n_jobs=-1)\n","  lr.fit(train_X, train_y)\n","  pred_y = lr.predict(val_X)\n","  \n","  lr_performance = report_results(pred_y, val_y)\n","  cv_acc.append(lr_performance[0])\n","  cv_pre.append(lr_performance[1])\n","  cv_rec.append(lr_performance[2])\n","  cv_f1.append(lr_performance[3])\n","\n","print('\\nLR + glove_200d')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ujY0TLHLXVNG"},"source":["**4. Random Forest + Count Vector & TF-IDF Vector & Word Embeddings**"]},{"cell_type":"code","metadata":{"id":"OjZH2tdzcJTZ"},"source":["##### count vector\n","\n","count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True)\n","train_val_X = count.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1) \n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  rf = RandomForestClassifier(n_estimators=500, random_state=0) # this is where your can change hyper-parameters for random forest \n","  rf.fit(train_X, train_y)\n","  pred_y = rf.predict(val_X)\n","  \n","  rf_performance = report_results(pred_y, val_y)\n","  cv_acc.append(rf_performance[0])\n","  cv_pre.append(rf_performance[1])\n","  cv_rec.append(rf_performance[2])\n","  cv_f1.append(rf_performance[3])\n","\n","print('\\nRF + count')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### TFIDF vector\n","\n","\n","tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","train_val_X = tfidf.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  rf = RandomForestClassifier(n_estimators=500, random_state=0)\n","  rf.fit(train_X, train_y)\n","  pred_y = rf.predict(val_X)\n","  \n","  rf_performance = report_results(pred_y, val_y)\n","  cv_acc.append(rf_performance[0])\n","  cv_pre.append(rf_performance[1])\n","  cv_rec.append(rf_performance[2])\n","  cv_f1.append(rf_performance[3])\n","\n","print('\\nRF + tfidf')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### glove\n","\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","embeddings_index = {}\n","f = open('glove.6B.200d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","glove_X = [sent2vec(x) for x in tqdm(df[\"text\"])]\n","glove_X = np.array(glove_X)\n","y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  train_X = glove_X[train_index,:]\n","  val_X = glove_X[val_index,:]\n","\n","  train_y = y[train_index]\n","  val_y = y[val_index]\n","\n","  rf = RandomForestClassifier(n_estimators=500, random_state=0)\n","  rf.fit(train_X, train_y)\n","  pred_y = rf.predict(val_X)\n","  \n","  rf_performance = report_results(pred_y, val_y)\n","  cv_acc.append(rf_performance[0])\n","  cv_pre.append(rf_performance[1])\n","  cv_rec.append(rf_performance[2])\n","  cv_f1.append(rf_performance[3])\n","\n","print('\\nRF + glove_100d')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CIxwXo63Xfti"},"source":["**6. XGBoost + Count Vector & TF-IDF Vector & Word Embeddings**"]},{"cell_type":"code","metadata":{"id":"cpSW0JPJwh17"},"source":["##### count vector\n","\n","\n","count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True)\n","train_val_X = count.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  xgb = XGBClassifier()\n","  xgb.fit(train_X, train_y)\n","  pred_y = xgb.predict(val_X)\n","  \n","  xgb_performance = report_results(pred_y, val_y)\n","  cv_acc.append(xgb_performance[0])\n","  cv_pre.append(xgb_performance[1])\n","  cv_rec.append(xgb_performance[2])\n","  cv_f1.append(xgb_performance[3])\n","\n","print('\\nXGB + count')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### TFIDF vector\n","\n","\n","tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","train_val_X = tfidf.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  xgb = XGBClassifier() # this is where your can change hyper-parameters for XGBoost\n","  xgb.fit(train_X, train_y)\n","  pred_y = xgb.predict(val_X)\n","  \n","  xgb_performance = report_results(pred_y, val_y)\n","  cv_acc.append(xgb_performance[0])\n","  cv_pre.append(xgb_performance[1])\n","  cv_rec.append(xgb_performance[2])\n","  cv_f1.append(xgb_performance[3])\n","\n","print('\\nXGB + tfidf')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### glove\n","\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","embeddings_index = {}\n","f = open('glove.6B.200d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","\n","glove_X = [sent2vec(x) for x in tqdm(df[\"text\"])]\n","glove_X = np.array(glove_X)\n","y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  train_X = glove_X[train_index,:]\n","  val_X = glove_X[val_index,:]\n","\n","  train_y = y[train_index]\n","  val_y = y[val_index]\n","\n","  xgb = XGBClassifier()\n","  xgb.fit(train_X, train_y)\n","  pred_y = xgb.predict(val_X)\n","  \n","  xgb_performance = report_results(pred_y, val_y)\n","  cv_acc.append(xgb_performance[0])\n","  cv_pre.append(xgb_performance[1])\n","  cv_rec.append(xgb_performance[2])\n","  cv_f1.append(xgb_performance[3])\n","\n","print('\\nXGB + glove_200d')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))"],"execution_count":null,"outputs":[]}]}