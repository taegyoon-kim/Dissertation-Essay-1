{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"violent_political_rhetoric_classifier_traditional_models.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1FJJtvPusNumN7NhE2kFAv1gleu03GBSy","authorship_tag":"ABX9TyNSvnhXX0JU1Z9tXXxzzxU7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xjB3FOyEEbfm"},"source":["Author notes (Taegyoon Kim, taegyoon@psu.edu)\n","\n","---\n","\n","\n","- This is a notebook for classifiers for violent political rhetoric (https://osf.io/5ckw4/). \n","- The input data is tweet text that contains one or more of the violent keywords extracted using the violent keyword extractor (https://github.com/taegyoon-kim/violent_political_rheotric_on_twitter/blob/master/violent_political_rhetoric_violent_keyword_extract.py). \n","- The training data is available upon request via email. \n","- The notebook will be fully available upon publication of the paper\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PZMh6SGOWs1B"},"source":["Drive mount\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"roT24CLcQgsh"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z1QIBWXhEqJ_"},"source":["Packages\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"IOfXYk2GN19z"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn import model_selection\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split, cross_val_score \n","from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score\n","from sklearn.model_selection import cross_val_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8YypKtlW9cy"},"source":["Load data\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"WtBCLaSpOCM-"},"source":["url = '' # training set\n","\n","df = pd.read_csv(url)\n","df['text'] = df['status_final_text']\n","df['threat'] = df['final_binary'].astype(float)\n","df = df[['text','threat']]\n","\n","df = df.sample(frac=1).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y59x5GNpcKLe"},"source":["Evaluation metrics\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"rNjBNKgwcQoV"},"source":["from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n","\n","def report_results(A, B):\n","    \n","    df = pd.DataFrame({'A':A,\n","                       'B':B})\n","    df = df.dropna()\n","    A = df['A']\n","    B = df['B']\n","    \n","    acc = accuracy_score(B, A)\n","    f1 = f1_score(B, A)\n","    prec = precision_score(B, A)\n","    rec = recall_score(B, A)\n","\n","    performance = [acc, prec, rec, f1]\n","\n","    return performance"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qa6UUfrFXGHb"},"source":["Logistic regression + Count Vector & TF-IDF Vector & Word Embeddings\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"XF4JYfchSYc7"},"source":["##### count vector\n","\n","\n","count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True) # you can cahnge arguments here depending on how you want the text to be pre-processed/represented as a matrix\n","train_val_X = count.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1) # you set the number of folds for cross-validation\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  lr = LogisticRegression(C=1, random_state=7, solver='sag', max_iter=2000, n_jobs=-1) # this is where your can change hyper-parameters for logistic regression\n","  lr.fit(train_X, train_y)\n","  pred_y = lr.predict(val_X)\n","  \n","  lr_performance = report_results(pred_y, val_y)\n","  cv_acc.append(lr_performance[0])\n","  cv_pre.append(lr_performance[1])\n","  cv_rec.append(lr_performance[2])\n","  cv_f1.append(lr_performance[3])\n","\n","print('\\nLR + count')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### TFIDF vector\n","\n","\n","tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","train_val_X = tfidf.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  lr = LogisticRegression(C=1, random_state=7, solver='sag', max_iter=2000, n_jobs=-1)\n","  lr.fit(train_X, train_y)\n","  pred_y = lr.predict(val_X)\n","  \n","  lr_performance = report_results(pred_y, val_y)\n","  cv_acc.append(lr_performance[0])\n","  cv_pre.append(lr_performance[1])\n","  cv_rec.append(lr_performance[2])\n","  cv_f1.append(lr_performance[3])\n","\n","print('\\nLR + tfidf')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### glove\n","\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","embeddings_index = {}\n","f = open('glove.6B.200d.txt', encoding=\"utf8\")  # this is where glove embedding file is stored\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","\n","glove_X = [sent2vec(x) for x in tqdm(df[\"text\"])]\n","glove_X = np.array(glove_X)\n","y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  train_X = glove_X[train_index,:]\n","  val_X = glove_X[val_index,:]\n","\n","  train_y = y[train_index]\n","  val_y = y[val_index]\n","\n","  lr = LogisticRegression(C=1, random_state=7, solver='sag', max_iter=2000, n_jobs=-1)\n","  lr.fit(train_X, train_y)\n","  pred_y = lr.predict(val_X)\n","  \n","  lr_performance = report_results(pred_y, val_y)\n","  cv_acc.append(lr_performance[0])\n","  cv_pre.append(lr_performance[1])\n","  cv_rec.append(lr_performance[2])\n","  cv_f1.append(lr_performance[3])\n","\n","print('\\nLR + glove_200d')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ujY0TLHLXVNG"},"source":["Random Forest + Count Vector & TF-IDF Vector & Word Embeddings\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"OjZH2tdzcJTZ"},"source":["##### count vector\n","\n","count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True)\n","train_val_X = count.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1) \n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  rf = RandomForestClassifier(n_estimators=500, random_state=0) # this is where your can change hyper-parameters for random forest \n","  rf.fit(train_X, train_y)\n","  pred_y = rf.predict(val_X)\n","  \n","  rf_performance = report_results(pred_y, val_y)\n","  cv_acc.append(rf_performance[0])\n","  cv_pre.append(rf_performance[1])\n","  cv_rec.append(rf_performance[2])\n","  cv_f1.append(rf_performance[3])\n","\n","print('\\nRF + count')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### TFIDF vector\n","\n","\n","tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","train_val_X = tfidf.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  rf = RandomForestClassifier(n_estimators=500, random_state=0)\n","  rf.fit(train_X, train_y)\n","  pred_y = rf.predict(val_X)\n","  \n","  rf_performance = report_results(pred_y, val_y)\n","  cv_acc.append(rf_performance[0])\n","  cv_pre.append(rf_performance[1])\n","  cv_rec.append(rf_performance[2])\n","  cv_f1.append(rf_performance[3])\n","\n","print('\\nRF + tfidf')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### glove\n","\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","embeddings_index = {}\n","f = open('glove.6B.200d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","glove_X = [sent2vec(x) for x in tqdm(df[\"text\"])]\n","glove_X = np.array(glove_X)\n","y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  train_X = glove_X[train_index,:]\n","  val_X = glove_X[val_index,:]\n","\n","  train_y = y[train_index]\n","  val_y = y[val_index]\n","\n","  rf = RandomForestClassifier(n_estimators=500, random_state=0)\n","  rf.fit(train_X, train_y)\n","  pred_y = rf.predict(val_X)\n","  \n","  rf_performance = report_results(pred_y, val_y)\n","  cv_acc.append(rf_performance[0])\n","  cv_pre.append(rf_performance[1])\n","  cv_rec.append(rf_performance[2])\n","  cv_f1.append(rf_performance[3])\n","\n","print('\\nRF + glove_100d')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CIxwXo63Xfti"},"source":["XGBoost + Count Vector & TF-IDF Vector & Word Embeddings\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"cpSW0JPJwh17"},"source":["##### count vector\n","\n","\n","count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True)\n","train_val_X = count.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  xgb = XGBClassifier()\n","  xgb.fit(train_X, train_y)\n","  pred_y = xgb.predict(val_X)\n","  \n","  xgb_performance = report_results(pred_y, val_y)\n","  cv_acc.append(xgb_performance[0])\n","  cv_pre.append(xgb_performance[1])\n","  cv_rec.append(xgb_performance[2])\n","  cv_f1.append(xgb_performance[3])\n","\n","print('\\nXGB + count')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### TFIDF vector\n","\n","\n","tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","train_val_X = tfidf.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  xgb = XGBClassifier() # this is where your can change hyper-parameters for XGBoost\n","  xgb.fit(train_X, train_y)\n","  pred_y = xgb.predict(val_X)\n","  \n","  xgb_performance = report_results(pred_y, val_y)\n","  cv_acc.append(xgb_performance[0])\n","  cv_pre.append(xgb_performance[1])\n","  cv_rec.append(xgb_performance[2])\n","  cv_f1.append(xgb_performance[3])\n","\n","print('\\nXGB + tfidf')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","##### glove\n","\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","embeddings_index = {}\n","f = open('glove.6B.200d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","\n","glove_X = [sent2vec(x) for x in tqdm(df[\"text\"])]\n","glove_X = np.array(glove_X)\n","y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  train_X = glove_X[train_index,:]\n","  val_X = glove_X[val_index,:]\n","\n","  train_y = y[train_index]\n","  val_y = y[val_index]\n","\n","  xgb = XGBClassifier()\n","  xgb.fit(train_X, train_y)\n","  pred_y = xgb.predict(val_X)\n","  \n","  xgb_performance = report_results(pred_y, val_y)\n","  cv_acc.append(xgb_performance[0])\n","  cv_pre.append(xgb_performance[1])\n","  cv_rec.append(xgb_performance[2])\n","  cv_f1.append(xgb_performance[3])\n","\n","print('\\nXGB + glove_200d')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))"],"execution_count":null,"outputs":[]}]}