{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"violent_political_rhetoric_classifier_traditional_models.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1FJJtvPusNumN7NhE2kFAv1gleu03GBSy","authorship_tag":"ABX9TyMShTw/WcfSPLhwXcIs/wG3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PZMh6SGOWs1B"},"source":["**1. Import libraries**"]},{"cell_type":"code","metadata":{"id":"roT24CLcQgsh","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603268806399,"user_tz":-540,"elapsed":917,"user":{"displayName":"Taegyoon Kim","photoUrl":"","userId":"13025568659711700824"}},"outputId":"d2bcb958-d943-4d71-e7fe-075e24d4a95e"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IOfXYk2GN19z"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn import svm\n","from sklearn import model_selection\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split, cross_val_score \n","from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score\n","from sklearn.model_selection import cross_val_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8YypKtlW9cy"},"source":["**2. Load data**"]},{"cell_type":"code","metadata":{"id":"WtBCLaSpOCM-"},"source":["url = \"https://raw.githubusercontent.com/taegyoon-kim/Dissertation-Essay-1/master/training_sep21.csv\"\n","\n","df = pd.read_csv(url, error_bad_lines=False)\n","\n","df['text'] = df['status_final_text']\n","df['threat'] = df['final_binary'].astype(float)\n","df = df[['text','threat']]\n","\n","df = df.sample(frac=1).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y59x5GNpcKLe"},"source":["**3. Evaluation metrics**"]},{"cell_type":"code","metadata":{"id":"rNjBNKgwcQoV"},"source":["from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n","\n","def report_results(A, B):\n","    #A_name = A.name\n","    #B_name = B.name\n","    \n","    df = pd.DataFrame({'A':A,\n","                       'B':B})\n","    df = df.dropna()\n","    A = df['A']\n","    B = df['B']\n","    \n","    acc = accuracy_score(B, A)\n","    f1 = f1_score(B, A)\n","    prec = precision_score(B, A)\n","    rec = recall_score(B, A)\n","    \n","    #print('accuracy: %0.4f \\nprecision: %0.4f \\nrecall: %0.4f \\nF1 score: %0.4f' % (acc, prec, rec, f1))\n","\n","    performance = [acc, prec, rec, f1]\n","\n","    return performance\n","\n","#scoring = {'accuracy' : make_scorer(accuracy_score), 'precision' : make_scorer(precision_score), 'recall' : make_scorer(recall_score), 'f1_score' : make_scorer(f1_score)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qa6UUfrFXGHb"},"source":["**3. Logistic regression + Count Vector & TF-IDF Vector & Word Embeddings**"]},{"cell_type":"code","metadata":{"id":"XF4JYfchSYc7"},"source":["###############################################################\n","\n","\n","count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True)\n","train_val_X = count.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  lr = LogisticRegression(C=1, random_state=7, solver='sag', max_iter=2000, n_jobs=-1)\n","  lr.fit(train_X, train_y)\n","  pred_y = lr.predict(val_X)\n","  \n","  lr_performance = report_results(pred_y, val_y)\n","  cv_acc.append(lr_performance[0])\n","  cv_pre.append(lr_performance[1])\n","  cv_rec.append(lr_performance[2])\n","  cv_f1.append(lr_performance[3])\n","\n","print('\\nLR + count')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","###############################################################\n","\n","\n","tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","train_val_X = tfidf.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  lr = LogisticRegression(C=1, random_state=7, solver='sag', max_iter=2000, n_jobs=-1)\n","  lr.fit(train_X, train_y)\n","  pred_y = lr.predict(val_X)\n","  \n","  lr_performance = report_results(pred_y, val_y)\n","  cv_acc.append(lr_performance[0])\n","  cv_pre.append(lr_performance[1])\n","  cv_rec.append(lr_performance[2])\n","  cv_f1.append(lr_performance[3])\n","\n","print('\\nLR + tfidf')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","###############################################################\n","\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","embeddings_index = {}\n","f = open('/content/drive/My Drive/diss_detection/python_scripts/glove/glove.6B.200d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","for data in data_sets:\n","  glove_X = [sent2vec(x) for x in tqdm(data[\"text\"])]\n","  glove_X = np.array(glove_X)\n","  y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  train_X = glove_X[train_index,:]\n","  val_X = glove_X[val_index,:]\n","\n","  train_y = y[train_index]\n","  val_y = y[val_index]\n","\n","  lr = LogisticRegression(C=1, random_state=7, solver='sag', max_iter=2000, n_jobs=-1)\n","  lr.fit(train_X, train_y)\n","  pred_y = lr.predict(val_X)\n","  \n","  lr_performance = report_results(pred_y, val_y)\n","  cv_acc.append(lr_performance[0])\n","  cv_pre.append(lr_performance[1])\n","  cv_rec.append(lr_performance[2])\n","  cv_f1.append(lr_performance[3])\n","\n","print('\\nLR + glove_200d')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjZH2tdzcJTZ"},"source":["############################ RF ############################\n","\n","\n","count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True)\n","train_val_X = count.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  rf = RandomForestClassifier(n_estimators=500, random_state=0)\n","  rf.fit(train_X, train_y)\n","  pred_y = rf.predict(val_X)\n","  \n","  rf_performance = report_results(pred_y, val_y)\n","  cv_acc.append(rf_performance[0])\n","  cv_pre.append(rf_performance[1])\n","  cv_rec.append(rf_performance[2])\n","  cv_f1.append(rf_performance[3])\n","\n","print('\\nRF + count')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","###############################################################\n","\n","\n","tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","train_val_X = tfidf.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  rf = RandomForestClassifier(n_estimators=500, random_state=0)\n","  rf.fit(train_X, train_y)\n","  pred_y = rf.predict(val_X)\n","  \n","  rf_performance = report_results(pred_y, val_y)\n","  cv_acc.append(rf_performance[0])\n","  cv_pre.append(rf_performance[1])\n","  cv_rec.append(rf_performance[2])\n","  cv_f1.append(rf_performance[3])\n","\n","print('\\nRF + tfidf')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","###############################################################\n","\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","embeddings_index = {}\n","f = open('/content/drive/My Drive/diss_detection/python_scripts/glove/glove.6B.200d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","for data in data_sets:\n","  glove_X = [sent2vec(x) for x in tqdm(data[\"text\"])]\n","  glove_X = np.array(glove_X)\n","  y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  train_X = glove_X[train_index,:]\n","  val_X = glove_X[val_index,:]\n","\n","  train_y = y[train_index]\n","  val_y = y[val_index]\n","\n","  rf = RandomForestClassifier(n_estimators=500, random_state=0)\n","  rf.fit(train_X, train_y)\n","  pred_y = rf.predict(val_X)\n","  \n","  rf_performance = report_results(pred_y, val_y)\n","  cv_acc.append(rf_performance[0])\n","  cv_pre.append(rf_performance[1])\n","  cv_rec.append(rf_performance[2])\n","  cv_f1.append(rf_performance[3])\n","\n","print('\\nRF + glove_100d')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cpSW0JPJwh17"},"source":["###############################################################XGB\n","\n","\n","count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True)\n","train_val_X = count.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  xgb = XGBClassifier()\n","  xgb.fit(train_X, train_y)\n","  pred_y = xgb.predict(val_X)\n","  \n","  xgb_performance = report_results(pred_y, val_y)\n","  cv_acc.append(xgb_performance[0])\n","  cv_pre.append(xgb_performance[1])\n","  cv_rec.append(xgb_performance[2])\n","  cv_f1.append(xgb_performance[3])\n","\n","print('\\nXGB + count')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","###############################################################\n","\n","\n","tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","train_val_X = tfidf.fit_transform(df['text'])\n","train_val_y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  \n","  train_X = train_val_X[train_index,:]\n","  val_X = train_val_X[val_index,:]\n","\n","  train_y = train_val_y[train_index]\n","  val_y = train_val_y[val_index]\n","\n","  xgb = XGBClassifier()\n","  xgb.fit(train_X, train_y)\n","  pred_y = xgb.predict(val_X)\n","  \n","  xgb_performance = report_results(pred_y, val_y)\n","  cv_acc.append(xgb_performance[0])\n","  cv_pre.append(xgb_performance[1])\n","  cv_rec.append(xgb_performance[2])\n","  cv_f1.append(xgb_performance[3])\n","\n","print('\\nXGB + tfidf')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))\n","\n","\n","###############################################################\n","\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","embeddings_index = {}\n","f = open('/content/drive/My Drive/diss_detection/python_scripts/glove/glove.6B.200d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","for data in data_sets:\n","  glove_X = [sent2vec(x) for x in tqdm(data[\"text\"])]\n","  glove_X = np.array(glove_X)\n","  y = df['threat']\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = True, random_state= 1)\n","\n","cv_acc = []\n","cv_pre= []\n","cv_rec = []\n","cv_f1 = []\n","\n","for train_index, val_index in kf.split(train_val_X):\n","  train_X = glove_X[train_index,:]\n","  val_X = glove_X[val_index,:]\n","\n","  train_y = y[train_index]\n","  val_y = y[val_index]\n","\n","  xgb = XGBClassifier()\n","  xgb.fit(train_X, train_y)\n","  pred_y = xgb.predict(val_X)\n","  \n","  xgb_performance = report_results(pred_y, val_y)\n","  cv_acc.append(xgb_performance[0])\n","  cv_pre.append(xgb_performance[1])\n","  cv_rec.append(xgb_performance[2])\n","  cv_f1.append(xgb_performance[3])\n","\n","print('\\nXGB + glove_200d')\n","print('- acc:', round(np.mean(cv_acc)*100,2))\n","print('- pre:', round(np.mean(cv_pre)*100,2))\n","print('- rec:', round(np.mean(cv_rec)*100,2))\n","print('- f1:', round(np.mean(cv_f1)*100,2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4OB0oTTbXPg8"},"source":["# count\n","\n","for data in data_sets:\n","  Count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True)\n","  Count_X = Count.fit_transform(data[\"text\"])\n","  y = data['threat']\n","  lr_results = model_selection.cross_validate(estimator=lr,\n","                                              X=Count_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports (K=5) for logistic regression + count','\\n', \n","        'accuracy:', round(100*lr_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(lr_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*lr_results['test_precision'].mean(), 2),'/ Std:',round(np.std(lr_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*lr_results['test_recall'].mean(), 2),'/ Std:',round(np.std(lr_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*lr_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(lr_results['test_f1_score']), 2))\n","  \n","\n","# TF-IDF\n","\n","for data in data_sets:\n","  Tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","  tfidf_X = Tfidf.fit_transform(data[\"text\"])\n","  y = data['threat']\n","  lr_results = model_selection.cross_validate(estimator=lr,\n","                                              X=tfidf_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports (K=5) for logistic regression + TF-IDF','\\n', \n","        'accuracy:', round(100*lr_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(lr_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*lr_results['test_precision'].mean(), 2),'/ Std:',round(np.std(lr_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*lr_results['test_recall'].mean(), 2),'/ Std:',round(np.std(lr_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*lr_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(lr_results['test_f1_score']), 2))\n","\n","\n","# GloVe\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","embeddings_index = {}\n","f = open('/content/drive/My Drive/diss_detection/python_scripts/glove/glove.6B.200d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","for data in data_sets:\n","  glove_X = [sent2vec(x) for x in tqdm(data[\"text\"])]\n","  glove_X = np.array(glove_X)\n","  y = data['threat']\n","  lr_results = model_selection.cross_validate(estimator=lr,\n","                                              X=glove_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports (K=5) for logistic regression + GloVe','\\n', \n","        'accuracy:', round(100*lr_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(lr_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*lr_results['test_precision'].mean(), 2),'/ Std:',round(np.std(lr_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*lr_results['test_recall'].mean(), 2),'/ Std:',round(np.std(lr_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*lr_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(lr_results['test_f1_score']), 2))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8U8N3ZumBCi"},"source":["**4. Random Forest + Count Vector & TF-IDF Vector & Word Embeddings**"]},{"cell_type":"code","metadata":{"id":"gGYaNj3skT_9"},"source":["kfold = model_selection.KFold(n_splits=5, random_state=42, shuffle = True)\n","rf = RandomForestClassifier(n_estimators=500, random_state=0)\n","data_sets = [df]\n","\n","# count\n","for data in data_sets:\n","  Count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True)\n","  Count_X = Count.fit_transform(data[\"text\"])\n","  y = data['threat']\n","  rf_results = model_selection.cross_validate(estimator=rf,\n","                                              X=Count_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports (K=5) for random forest + count','\\n', \n","        'accuracy:', round(100*rf_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(rf_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*rf_results['test_precision'].mean(), 2),'/ Std:',round(np.std(rf_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*rf_results['test_recall'].mean(), 2),'/ Std:',round(np.std(rf_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*rf_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(rf_results['test_f1_score']), 2))\n","  \n","\n","# TF-IDF\n","for data in data_sets:\n","  Tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","  tfidf_X = Tfidf.fit_transform(data[\"text\"])\n","  y = data['threat']\n","  rf_results = model_selection.cross_validate(estimator=rf,\n","                                              X=tfidf_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports (K=5) for random forest + TF-IDF','\\n', \n","        'accuracy:', round(100*rf_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(rf_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*rf_results['test_precision'].mean(), 2),'/ Std:',round(np.std(rf_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*rf_results['test_recall'].mean(), 2),'/ Std:',round(np.std(rf_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*rf_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(rf_results['test_f1_score']), 2))\n","  \n","\n","# GloVe\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","\n","embeddings_index = {}\n","f = open('/content/drive/My Drive/diss_detection/python_scripts/glove/glove.6B.200d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","for data in data_sets:\n","  glove_X = [sent2vec(x) for x in tqdm(data[\"text\"])]\n","  glove_X = np.array(glove_X)\n","  y = data['threat']\n","  lr_results = model_selection.cross_validate(estimator=lr,\n","                                              X=glove_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports (K=5) for logistic regression + GloVe','\\n', \n","        'accuracy:', round(100*lr_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(lr_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*lr_results['test_precision'].mean(), 2),'/ Std:',round(np.std(lr_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*lr_results['test_recall'].mean(), 2),'/ Std:',round(np.std(lr_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*lr_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(lr_results['test_f1_score']), 2))\n","\n","\n","# GloVe\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","\n","embeddings_index = {}\n","f = open('/content/drive/My Drive/glove_twitter/glove.twitter.27B.100d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(100)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","for data in data_sets:\n","  glove_X = [sent2vec(x) for x in tqdm(data[\"text\"])]\n","  glove_X = np.array(glove_X)\n","  y = data['threat']\n","  rf_results = model_selection.cross_validate(estimator=rf,\n","                                              X=glove_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports (K=5) for random forest + GloVe','\\n', \n","        'accuracy:', round(100*rf_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(rf_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*rf_results['test_precision'].mean(), 2),'/ Std:',round(np.std(rf_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*rf_results['test_recall'].mean(), 2),'/ Std:',round(np.std(rf_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*rf_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(rf_results['test_f1_score']), 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jgS3HYwoSUc7"},"source":["***5. XGBoost + Count Vector & TF-IDF Vector***\n"]},{"cell_type":"code","metadata":{"id":"It71eqHdmDeT"},"source":["\n","\n","kfold = model_selection.KFold(n_splits=5, random_state=42, shuffle = True)\n","xgb = XGBClassifier()\n","data_sets = [df]\n","\n","# count\n","for data in data_sets:\n","  Count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True)\n","  Count_X = Count.fit_transform(data[\"text\"])\n","  y = data['threat']\n","  rf_results = model_selection.cross_validate(estimator=xgb,\n","                                              X=Count_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports for XGBoost + count (K=5)','\\n', \n","        'accuracy:', round(100*rf_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(rf_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*rf_results['test_precision'].mean(), 2),'/ Std:',round(np.std(rf_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*rf_results['test_recall'].mean(), 2),'/ Std:',round(np.std(rf_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*rf_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(rf_results['test_f1_score']), 2))\n","  \n","# TF-IDF\n","for data in data_sets:\n","  Tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","  Tfidf_X = Tfidf.fit_transform(data[\"text\"])\n","  y = data['threat']\n","  rf_results = model_selection.cross_validate(estimator=xgb,\n","                                              X=Tfidf_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports for XGBoost + TF-IDF (K=5)','\\n', \n","        'accuracy:', round(100*rf_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(rf_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*rf_results['test_precision'].mean(), 2),'/ Std:',round(np.std(rf_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*rf_results['test_recall'].mean(), 2),'/ Std:',round(np.std(rf_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*rf_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(rf_results['test_f1_score']), 2))\n","  \n","\n","# GloVe\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","\n","embeddings_index = {}\n","f = open('/content/drive/My Drive/diss_detection/python_scripts/glove/glove.6B.100d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(100)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","for data in data_sets:\n","  glove_X = [sent2vec(x) for x in tqdm(data[\"text\"])]\n","  glove_X = np.array(glove_X)\n","  y = data['threat']\n","  xgb_results = model_selection.cross_validate(estimator=xgb,\n","                                              X=glove_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports (K=5) for XGBoost + GloVe','\\n', \n","        'accuracy:', round(100*xgb_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(xgb_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*xgb_results['test_precision'].mean(), 2),'/ Std:',round(np.std(xgb_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*xgb_results['test_recall'].mean(), 2),'/ Std:',round(np.std(xgb_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*xgb_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(xgb_results['test_f1_score']), 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hsWyyCCuLUzO"},"source":["***6. SVM + Count Vector & TF-IDF Vector***"]},{"cell_type":"code","metadata":{"id":"HGkAuOjJ2Xpk"},"source":["from sklearn import svm\n","kfold = model_selection.KFold(n_splits=5, random_state=3, shuffle = True)\n","svm = svm.SVC(kernel='linear', C = 1)\n","data_sets = [df]\n","for data in data_sets:\n","  Count = CountVectorizer(ngram_range = (1,2), binary = True, lowercase = True)\n","  Count_X = Count.fit_transform(data[\"text\"])\n","  y = data['threat']\n","  svm_results = model_selection.cross_validate(estimator=svm,\n","                                              X=Count_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports for support vector machine + count (K=5)','\\n', \n","        'accuracy:', round(100*svm_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(svm_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*svm_results['test_precision'].mean(), 2),'/ Std:',round(np.std(svm_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*svm_results['test_recall'].mean(), 2),'/ Std:',round(np.std(svm_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*svm_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(svm_results['test_f1_score']), 2))\n","  \n","for data in data_sets:\n","  Tfidf = TfidfVectorizer(ngram_range = (1,2), lowercase = True)\n","  Tfidf_X = Tfidf.fit_transform(data[\"text\"])\n","  y = data['threat']\n","  svm_results = model_selection.cross_validate(estimator=svm,\n","                                              X=Tfidf_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports for support vector machine + TF-IDF(K=5)','\\n', \n","        'accuracy:', round(100*svm_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(svm_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*svm_results['test_precision'].mean(), 2),'/ Std:',round(np.std(svm_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*svm_results['test_recall'].mean(), 2),'/ Std:',round(np.std(svm_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*svm_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(svm_results['test_f1_score']), 2))\n","\n","# GloVe\n","\n","from tqdm import tqdm\n","import nltk\n","from nltk import word_tokenize\n","from nltk import punkt\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","\n","\n","embeddings_index = {}\n","f = open('/content/drive/My Drive/glove_twitter/glove.twitter.27B.200d.txt', encoding=\"utf8\")\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    try:\n","       coefs = np.asarray(values[1:], dtype='float32')\n","       embeddings_index[word] = coefs\n","    except ValueError:\n","       pass\n","f.close()\n","\n","def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(200)\n","    return v / np.sqrt((v ** 2).sum())\n","\n","for data in data_sets:\n","  glove_X = [sent2vec(x) for x in tqdm(data[\"text\"])]\n","  glove_X = np.array(glove_X)\n","  y = data['threat']\n","  svm_results = model_selection.cross_validate(estimator=svm,\n","                                              X=glove_X,\n","                                              y=y,\n","                                              cv=kfold,\n","                                              scoring=scoring)\n","  print('cross-validation reports (K=5) for SVM + GloVe','\\n', \n","        'accuracy:', round(100*svm_results['test_accuracy'].mean(),2),'/ Std:',round(np.std(svm_results['test_accuracy']), 2),'\\n', \n","        'precision:', round(100*svm_results['test_precision'].mean(), 2),'/ Std:',round(np.std(svm_results['test_precision']), 2),'\\n', \n","        'recall:', round(100*svm_results['test_recall'].mean(), 2),'/ Std:',round(np.std(svm_results['test_recall']), 2),'\\n', \n","        'F-1:', round(100*svm_results['test_f1_score'].mean(), 2), '/ Std:',round(np.std(svm_results['test_f1_score']), 2))"],"execution_count":null,"outputs":[]}]}